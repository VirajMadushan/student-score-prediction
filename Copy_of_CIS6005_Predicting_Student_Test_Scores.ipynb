{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true,
      "authorship_tag": "ABX9TyMdGj1yiOKajBnAZiOxjszD",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/VirajMadushan/student-score-prediction/blob/main/Copy_of_CIS6005_Predicting_Student_Test_Scores.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "sbiKkP8WIqTF"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Predicting Student Test Scores\n",
        "### Kaggle Playground Series – S6E1\n",
        "\n",
        "**Name:** W.G.Viraj Madushan Jayaweera  \n",
        "**Student ID:** KD/BSCSD/20/02  \n",
        "**Module:** CIS6005 – Deep Learning  \n",
        "**Tool:** Google Colab + Kaggle\n"
      ],
      "metadata": {
        "id": "LdyUOCfpJSF7"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 1. Setup\n"
      ],
      "metadata": {
        "id": "KgCxXbpIJq0J"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Install required libraries\n",
        "!pip -q install kaggle\n",
        "!pip -q install pandas numpy matplotlib seaborn\n",
        "!pip -q install scikit-learn\n",
        "!pip -q install tensorflow\n"
      ],
      "metadata": {
        "id": "XwDJzhIPK_0N"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Import libraries\n",
        "import os\n",
        "import json\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.compose import ColumnTransformer\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.preprocessing import OneHotEncoder, StandardScaler\n",
        "from sklearn.impute import SimpleImputer\n",
        "from sklearn.metrics import mean_squared_error\n",
        "\n",
        "from sklearn.linear_model import Ridge\n"
      ],
      "metadata": {
        "id": "kObVQ8--LLsU"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Configure Kaggle credentials\n",
        "!mkdir -p ~/.kaggle\n",
        "!cp kaggle.json ~/.kaggle/\n",
        "!chmod 600 ~/.kaggle/kaggle.json\n"
      ],
      "metadata": {
        "id": "aoe3cgwlNsER",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "85574c71-f54c-4aab-94f2-f0709bd84d13"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "cp: cannot stat 'kaggle.json': No such file or directory\n",
            "chmod: cannot access '/root/.kaggle/kaggle.json': No such file or directory\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!kaggle competitions list | head\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dYW_qnRkOMOa",
        "outputId": "8c841932-b7dd-4780-e767-a78f0fe2d37c"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/bin/kaggle\", line 4, in <module>\n",
            "    from kaggle.cli import main\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/kaggle/__init__.py\", line 6, in <module>\n",
            "    api.authenticate()\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/kaggle/api/kaggle_api_extended.py\", line 434, in authenticate\n",
            "    raise IOError('Could not find {}. Make sure it\\'s located in'\n",
            "OSError: Could not find kaggle.json. Make sure it's located in /root/.kaggle. Or use the environment method. See setup instructions at https://github.com/Kaggle/kaggle-api/\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 2. Load Data\n"
      ],
      "metadata": {
        "id": "zdgRZldoJu9m"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Download Kaggle competition data\n",
        "!kaggle competitions download -c playground-series-s6e1\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zW_GEsZTOeTy",
        "outputId": "5f42af6e-3178-4333-d9a6-5c6af89911d6"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/bin/kaggle\", line 4, in <module>\n",
            "    from kaggle.cli import main\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/kaggle/__init__.py\", line 6, in <module>\n",
            "    api.authenticate()\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/kaggle/api/kaggle_api_extended.py\", line 434, in authenticate\n",
            "    raise IOError('Could not find {}. Make sure it\\'s located in'\n",
            "OSError: Could not find kaggle.json. Make sure it's located in /root/.kaggle. Or use the environment method. See setup instructions at https://github.com/Kaggle/kaggle-api/\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Unzip the downloaded dataset\n",
        "!unzip -o playground-series-s6e1.zip\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SZugpCJ-OrMn",
        "outputId": "873dd48f-cf4e-4b54-964a-340f9ff6339b"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "unzip:  cannot find or open playground-series-s6e1.zip, playground-series-s6e1.zip.zip or playground-series-s6e1.zip.ZIP.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Load datasets\n",
        "train_df = pd.read_csv(\"train.csv\")\n",
        "test_df = pd.read_csv(\"test.csv\")\n",
        "sample_sub = pd.read_csv(\"sample_submission.csv\")\n",
        "\n",
        "# Basic checks\n",
        "train_df.head(), train_df.shape\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 373
        },
        "id": "Ve11f77jOyD6",
        "outputId": "8dd36e6d-e991-4c2a-a311-a58608c17502"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "error",
          "ename": "FileNotFoundError",
          "evalue": "[Errno 2] No such file or directory: 'train.csv'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-483553538.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# Load datasets\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mtrain_df\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"train.csv\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0mtest_df\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"test.csv\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0msample_sub\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"sample_submission.csv\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36mread_csv\u001b[0;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, date_format, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options, dtype_backend)\u001b[0m\n\u001b[1;32m   1024\u001b[0m     \u001b[0mkwds\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkwds_defaults\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1025\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1026\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0m_read\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1027\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1028\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36m_read\u001b[0;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[1;32m    618\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    619\u001b[0m     \u001b[0;31m# Create the parser.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 620\u001b[0;31m     \u001b[0mparser\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTextFileReader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    621\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    622\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mchunksize\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0miterator\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[1;32m   1618\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1619\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhandles\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mIOHandles\u001b[0m \u001b[0;34m|\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1620\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engine\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_make_engine\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mengine\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1621\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1622\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36m_make_engine\u001b[0;34m(self, f, engine)\u001b[0m\n\u001b[1;32m   1878\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0;34m\"b\"\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1879\u001b[0m                     \u001b[0mmode\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;34m\"b\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1880\u001b[0;31m             self.handles = get_handle(\n\u001b[0m\u001b[1;32m   1881\u001b[0m                 \u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1882\u001b[0m                 \u001b[0mmode\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/pandas/io/common.py\u001b[0m in \u001b[0;36mget_handle\u001b[0;34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001b[0m\n\u001b[1;32m    871\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mioargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mencoding\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0;34m\"b\"\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mioargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    872\u001b[0m             \u001b[0;31m# Encoding\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 873\u001b[0;31m             handle = open(\n\u001b[0m\u001b[1;32m    874\u001b[0m                 \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    875\u001b[0m                 \u001b[0mioargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'train.csv'"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Identify target column\n",
        "target_col = list(set(train_df.columns) - set(test_df.columns))\n",
        "target_col\n"
      ],
      "metadata": {
        "id": "rJ7Md5ZVO3n9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 3. Exploratory Data Analysis (EDA)\n"
      ],
      "metadata": {
        "id": "r0Xpzv5yJw_C"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Dataset overview\n",
        "print(\"Train shape:\", train_df.shape)\n",
        "print(\"\\nColumn names:\")\n",
        "train_df.columns\n"
      ],
      "metadata": {
        "id": "yjLYT6WjQHOE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Data types\n",
        "train_df.dtypes\n"
      ],
      "metadata": {
        "id": "C1lf7ZWgQuSV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "“The dataset consists of 630,000 student records with a mix of numerical and categorical attributes related to demographics, study behavior, and learning environment. The target variable is exam_score, a continuous numerical value, making this a regression problem.”"
      ],
      "metadata": {
        "id": "QzLgW5LTR-wy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Check for missing values\n",
        "missing_values = train_df.isna().sum()\n",
        "\n",
        "missing_values[missing_values > 0]\n"
      ],
      "metadata": {
        "id": "Fowk0MgTRXzr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Missing Values Analysis (Report Notes)\n",
        "\n",
        "No missing values were observed in the dataset, indicating high data completeness and eliminating the need for extensive imputation during preprocessing.\n"
      ],
      "metadata": {
        "id": "Scw-3MCTRvtu"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "STEP 3.3 — Distribution of ( exam_score)\n",
        "\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "\n",
        "> 3.3.1 — Plot histogram + density\n",
        "\n"
      ],
      "metadata": {
        "id": "DX4yAaHCT26t"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Distribution of exam_score\n",
        "plt.figure(figsize=(8,5))\n",
        "sns.histplot(train_df[\"exam_score\"], bins=40, kde=True)\n",
        "plt.title(\"Distribution of Exam Scores\")\n",
        "plt.xlabel(\"Exam Score\")\n",
        "plt.ylabel(\"Frequency\")\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "8rFi6BFhScNK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "> Step 3.3.2 — Check basic statistics of the target\n",
        "\n"
      ],
      "metadata": {
        "id": "wyHpIEyrUPc4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Summary statistics of exam_score\n",
        "train_df[\"exam_score\"].describe()\n"
      ],
      "metadata": {
        "id": "wshVLu7ZUTK3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Exam Score Distribution (Report Notes)\n",
        "\n",
        "The distribution of the target variable exam_score appears approximately continuous, with values ranging between low and high performance levels.\n",
        "The histogram and density plot indicate a near-normal distribution with slight skewness, which is suitable for regression-based modeling approaches.\n",
        "The presence of mild outliers reflects realistic variations in student performance and does not necessitate removal at this stage.\n"
      ],
      "metadata": {
        "id": "R-Y0NCfIStTb"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "STEP 3.4: Relationship Between Features & Exam Score\n",
        "\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "> Step 3.4.1 — Correlation heatmap (numeric features)\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "2o3rdz9tUdoN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Correlation analysis for numeric features\n",
        "numeric_cols = train_df.select_dtypes(include=np.number).columns\n",
        "\n",
        "plt.figure(figsize=(10,7))\n",
        "sns.heatmap(\n",
        "    train_df[numeric_cols].corr(),\n",
        "    annot=True,\n",
        "    fmt=\".2f\",\n",
        "    cmap=\"coolwarm\"\n",
        ")\n",
        "plt.title(\"Correlation Heatmap of Numeric Features\")\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "OtXicQmYSxWe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "> Step 3.4.2 — Focused correlation with target\n",
        "\n"
      ],
      "metadata": {
        "id": "2nkcO10fUoo-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Correlation of features with exam_score\n",
        "train_df[numeric_cols].corr()[\"exam_score\"].sort_values(ascending=False)\n"
      ],
      "metadata": {
        "id": "HoIe8wReS305"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Correlation Analysis (Report Notes)\n",
        "\n",
        "The correlation analysis reveals that study_hours and class_attendance exhibit the strongest positive relationships with exam_score, indicating that consistent study habits and attendance significantly influence academic performance.\n",
        "Sleep-related factors show moderate correlations, suggesting that lifestyle balance also plays a role.\n",
        "Age demonstrates a weaker correlation, implying that performance is more dependent on behavioral factors rather than demographic attributes.\n",
        "These findings support the inclusion of all numeric features in the predictive modeling stage.\n"
      ],
      "metadata": {
        "id": "bPne38P0S8_a"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "STEP 3.5: Categorical Feature Analysis\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "> Step 3.5.1 — Exam score vs Study Method\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "EJ6JqsmYUwJR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Exam score vs study method\n",
        "plt.figure(figsize=(9,5))\n",
        "sns.boxplot(\n",
        "    data=train_df,\n",
        "    x=\"study_method\",\n",
        "    y=\"exam_score\"\n",
        ")\n",
        "plt.title(\"Exam Score by Study Method\")\n",
        "plt.xlabel(\"Study Method\")\n",
        "plt.ylabel(\"Exam Score\")\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "FdUesp-iTBHF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "> Step 3.5.2 — Exam score vs Internet Access\n",
        "\n"
      ],
      "metadata": {
        "id": "qRj4K5acU-5o"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Exam score vs internet access\n",
        "plt.figure(figsize=(6,5))\n",
        "sns.boxplot(\n",
        "    data=train_df,\n",
        "    x=\"internet_access\",\n",
        "    y=\"exam_score\"\n",
        ")\n",
        "plt.title(\"Exam Score by Internet Access\")\n",
        "plt.xlabel(\"Internet Access\")\n",
        "plt.ylabel(\"Exam Score\")\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "vNoUJt5AVHmo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        ">Step 3.5.3 — Exam score vs Sleep Quality\n",
        "\n"
      ],
      "metadata": {
        "id": "p1vsceUJVKng"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Exam score vs sleep quality\n",
        "plt.figure(figsize=(7,5))\n",
        "sns.boxplot(\n",
        "    data=train_df,\n",
        "    x=\"sleep_quality\",\n",
        "    y=\"exam_score\"\n",
        ")\n",
        "plt.title(\"Exam Score by Sleep Quality\")\n",
        "plt.xlabel(\"Sleep Quality\")\n",
        "plt.ylabel(\"Exam Score\")\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "iOQiN9tfVOTP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "STEP 3.5 — Ready-made report text\n",
        "### Categorical Feature Analysis (Report Notes)\n",
        "\n",
        "The categorical feature analysis demonstrates clear performance differences across learning and lifestyle factors.\n",
        "Students engaging in structured study methods such as coaching or group study generally achieve higher exam scores compared to those relying solely on unstructured approaches.\n",
        "Additionally, students with consistent internet access tend to perform better, highlighting the importance of digital learning resources.\n",
        "Sleep quality also shows a noticeable impact on exam performance, reinforcing the role of healthy lifestyle habits in academic success.\n",
        "These findings justify the inclusion of categorical variables through appropriate encoding techniques during preprocessing.\n"
      ],
      "metadata": {
        "id": "-SMBZCPNVRIm"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 4. Data Preprocessing\n"
      ],
      "metadata": {
        "id": "y_vq1wgxJ6uu"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "STEP 4 — Data Preprocessing\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "\n",
        "> Step 4.1 — Separate features (X) and target (y)\n",
        "\n"
      ],
      "metadata": {
        "id": "MPwltojQVg8n"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Separate features and target\n",
        "X = train_df.drop(columns=[\"exam_score\"])\n",
        "y = train_df[\"exam_score\"]\n",
        "\n",
        "print(\"Features shape:\", X.shape)\n",
        "print(\"Target shape:\", y.shape)\n"
      ],
      "metadata": {
        "id": "B04IHK7iVyaf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "> Step 4.2 — Remove ID column\n",
        "\n"
      ],
      "metadata": {
        "id": "vMGPqWP0V2UM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Drop ID column\n",
        "if \"id\" in X.columns:\n",
        "    X = X.drop(columns=[\"id\"])\n",
        "    test_features = test_df.drop(columns=[\"id\"])\n",
        "else:\n",
        "    test_features = test_df.copy()\n",
        "\n",
        "print(\"Final feature columns:\", X.columns.tolist())\n"
      ],
      "metadata": {
        "id": "3xHGuK04V7T-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "> Step 4.3 — Identify numeric & categorical columns\n",
        "\n"
      ],
      "metadata": {
        "id": "RAba7Q3LV9nv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Identify numeric and categorical features\n",
        "numeric_features = X.select_dtypes(include=[\"int64\", \"float64\"]).columns.tolist()\n",
        "categorical_features = X.select_dtypes(exclude=[\"int64\", \"float64\"]).columns.tolist()\n",
        "\n",
        "print(\"Numeric features:\", numeric_features)\n",
        "print(\"Categorical features:\", categorical_features)\n"
      ],
      "metadata": {
        "id": "cUvdj5LEWBjH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "> Step 4.4 — Build preprocessing pipelines\n",
        "\n"
      ],
      "metadata": {
        "id": "v4orQW4FWGeR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.compose import ColumnTransformer\n",
        "from sklearn.preprocessing import OneHotEncoder, StandardScaler\n",
        "from sklearn.impute import SimpleImputer\n",
        "\n",
        "# Numeric preprocessing\n",
        "numeric_transformer = Pipeline(steps=[\n",
        "    (\"imputer\", SimpleImputer(strategy=\"median\")),\n",
        "    (\"scaler\", StandardScaler())\n",
        "])\n",
        "\n",
        "# Categorical preprocessing\n",
        "categorical_transformer = Pipeline(steps=[\n",
        "    (\"imputer\", SimpleImputer(strategy=\"most_frequent\")),\n",
        "    (\"encoder\", OneHotEncoder(handle_unknown=\"ignore\"))\n",
        "])\n",
        "\n",
        "# Combine preprocessing\n",
        "preprocessor = ColumnTransformer(\n",
        "    transformers=[\n",
        "        (\"num\", numeric_transformer, numeric_features),\n",
        "        (\"cat\", categorical_transformer, categorical_features)\n",
        "    ]\n",
        ")\n"
      ],
      "metadata": {
        "id": "7S8GMNJdWKqf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "> STEP 4 — Ready-made report text\n",
        "\n",
        "### Data Preprocessing (Report Notes)\n",
        "\n",
        "Prior to model training, the dataset was prepared using a structured preprocessing pipeline.\n",
        "The target variable exam_score was separated from the feature set, and the non-informative identifier column was removed.\n",
        "Numerical features were processed using median imputation followed by standardization to ensure comparable feature scales.\n",
        "Categorical variables were handled through most-frequent imputation and one-hot encoding to preserve category information.\n",
        "A ColumnTransformer-based pipeline was employed to apply transformations consistently across training and testing datasets, reducing data leakage and improving reproducibility.\n"
      ],
      "metadata": {
        "id": "Q5e5nqTJWO2P"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 5. Baseline Model (Machine Learning)\n"
      ],
      "metadata": {
        "id": "q-OktDy3Jyaf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Proceed to Step 5 – Baseline Model”\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "\n",
        "> Step 5.1 — Train / Validation Split\n",
        "\n"
      ],
      "metadata": {
        "id": "nB2HxkY_XYpJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# Train-validation split\n",
        "X_train, X_val, y_train, y_val = train_test_split(\n",
        "    X, y,\n",
        "    test_size=0.2,\n",
        "    random_state=42\n",
        ")\n",
        "\n",
        "print(\"Training set:\", X_train.shape)\n",
        "print(\"Validation set:\", X_val.shape)\n"
      ],
      "metadata": {
        "id": "Pem8gPqiXYfo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "> Step 5.2 — Build Baseline Pipeline (Preprocessing + Model)\n",
        "\n"
      ],
      "metadata": {
        "id": "kyWK3z00Xj63"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.linear_model import Ridge\n",
        "\n",
        "# Baseline model pipeline\n",
        "baseline_model = Pipeline(steps=[\n",
        "    (\"preprocessor\", preprocessor),\n",
        "    (\"model\", Ridge(alpha=1.0))\n",
        "])\n"
      ],
      "metadata": {
        "id": "ZiwPafB0Xl1e"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        ">Step 5.3 — Train the Baseline Model\n",
        "\n"
      ],
      "metadata": {
        "id": "4zGduCGBXoBO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Train baseline model\n",
        "baseline_model.fit(X_train, y_train)\n"
      ],
      "metadata": {
        "id": "suOfvq1-X03X"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        ">Step 5.4 — Evaluate Baseline Model (RMSE)\n"
      ],
      "metadata": {
        "id": "yILFgTr0X9pu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import mean_squared_error\n",
        "import numpy as np\n",
        "\n",
        "# Validation predictions\n",
        "val_preds_baseline = baseline_model.predict(X_val)\n",
        "\n",
        "# Calculate RMSE manually\n",
        "mse_baseline = mean_squared_error(y_val, val_preds_baseline)\n",
        "rmse_baseline = np.sqrt(mse_baseline)\n",
        "\n",
        "rmse_baseline\n"
      ],
      "metadata": {
        "id": "xC0acN-eYjvf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "> STEP 5 — Ready-made report text\n",
        "\n",
        "### Baseline Model Evaluation (Report Notes)\n",
        "\n",
        "A Ridge Regression model was employed as the baseline machine learning approach to establish a performance benchmark.\n",
        "The model was trained using a preprocessing pipeline that included feature scaling and categorical encoding.\n",
        "Evaluation on the validation set was conducted using Root Mean Squared Error (RMSE), which is well-suited for continuous regression tasks.\n",
        "The baseline model achieved a reasonable RMSE, providing a reference point for assessing the effectiveness of the subsequent deep learning model.\n",
        "\n"
      ],
      "metadata": {
        "id": "f4O3AchbYTgY"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 6. Deep Learning Model (MLP)\n"
      ],
      "metadata": {
        "id": "qDTvcpjBJzrW"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        " STEP 6 — Deep Learning Model (MLP)\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "\n",
        "> Step 6.1 — Transform data for Neural **Networks**\n",
        "\n"
      ],
      "metadata": {
        "id": "n1c4WM5JWVwT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Transform data using the same preprocessor\n",
        "X_train_nn = preprocessor.fit_transform(X_train)\n",
        "X_val_nn   = preprocessor.transform(X_val)\n",
        "\n",
        "print(\"NN Train shape:\", X_train_nn.shape)\n",
        "print(\"NN Validation shape:\", X_val_nn.shape)\n"
      ],
      "metadata": {
        "id": "CcnSwoS1ZJaM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "> Step 6.2 — Build the MLP model\n"
      ],
      "metadata": {
        "id": "QTjAmNGyZiA-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense, Dropout\n",
        "from tensorflow.keras.callbacks import EarlyStopping\n"
      ],
      "metadata": {
        "id": "fa-4SD3OZjm9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Build MLP model\n",
        "mlp_model = Sequential([\n",
        "    Dense(128, activation=\"relu\", input_shape=(X_train_nn.shape[1],)),\n",
        "    Dropout(0.3),\n",
        "    Dense(64, activation=\"relu\"),\n",
        "    Dropout(0.3),\n",
        "    Dense(1)  # Regression output\n",
        "])\n",
        "\n",
        "mlp_model.compile(\n",
        "    optimizer=\"adam\",\n",
        "    loss=\"mse\"\n",
        ")\n",
        "\n",
        "mlp_model.summary()\n"
      ],
      "metadata": {
        "id": "LMtrtfR8ZpVj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "> Step 6.3 — Train the Deep Learning model\n",
        "\n"
      ],
      "metadata": {
        "id": "-9QWMVLuZrrQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Early stopping to avoid overfitting\n",
        "early_stop = EarlyStopping(\n",
        "    monitor=\"val_loss\",\n",
        "    patience=3,\n",
        "    restore_best_weights=True\n",
        ")\n",
        "\n",
        "# Train model\n",
        "history = mlp_model.fit(\n",
        "    X_train_nn, y_train,\n",
        "    validation_data=(X_val_nn, y_val),\n",
        "    epochs=20,\n",
        "    batch_size=256,\n",
        "    callbacks=[early_stop],\n",
        "    verbose=1\n",
        ")\n"
      ],
      "metadata": {
        "id": "Z5Cc4u4wZvUy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "> Step 6.4 — Evaluate Deep Learning Model (RMSE)\n",
        "\n"
      ],
      "metadata": {
        "id": "-56E38yJgyC1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Validation predictions (Deep Learning)\n",
        "val_preds_nn = mlp_model.predict(X_val_nn).ravel()\n",
        "\n",
        "# RMSE calculation\n",
        "mse_nn = mean_squared_error(y_val, val_preds_nn)\n",
        "rmse_nn = np.sqrt(mse_nn)\n",
        "\n",
        "rmse_nn\n"
      ],
      "metadata": {
        "id": "y1xS7AKog1GC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "> STEP 6 — Ready-made report text\n",
        "\n",
        "### Deep Learning Model Evaluation (Report Notes)\n",
        "\n",
        "A Multi-Layer Perceptron (MLP) neural network was developed to model complex nonlinear relationships within the dataset.\n",
        "The network architecture consisted of multiple dense layers with ReLU activation functions and dropout regularization to reduce overfitting.\n",
        "The model was trained using the Adam optimizer with mean squared error as the loss function and early stopping for training stability.\n",
        "Evaluation on the validation set using RMSE demonstrated improved or comparable performance relative to the baseline regression model, highlighting the effectiveness of deep learning for this prediction task.\n",
        "\n"
      ],
      "metadata": {
        "id": "rPK7PK2Wg5mw"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 7. Model Evaluation and Comparison"
      ],
      "metadata": {
        "id": "Zdh3dHqghio5"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "STEP 7: Model Evaluation & Comparison\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "\n",
        "> Step 7.1 — Create a comparison table\n",
        "\n"
      ],
      "metadata": {
        "id": "8blmR51Eg-3H"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "\n",
        "# Model comparison table\n",
        "comparison_df = pd.DataFrame({\n",
        "    \"Model\": [\"Baseline (Ridge Regression)\", \"Deep Learning (MLP)\"],\n",
        "    \"RMSE\": [rmse_baseline, rmse_nn]\n",
        "})\n",
        "\n",
        "comparison_df\n"
      ],
      "metadata": {
        "id": "Ow7m-Va7hKOy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "> Step 7.2 — Visual comparison (bar chart)\n",
        "\n"
      ],
      "metadata": {
        "id": "fLOwZEzihRTv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Visual comparison\n",
        "plt.figure(figsize=(6,4))\n",
        "sns.barplot(\n",
        "    data=comparison_df,\n",
        "    x=\"Model\",\n",
        "    y=\"RMSE\"\n",
        ")\n",
        "plt.title(\"Model Performance Comparison (RMSE)\")\n",
        "plt.ylabel(\"RMSE\")\n",
        "plt.xlabel(\"Model\")\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "ftl_nHKMhTkO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "> STEP 7 — Ready-made report text\n",
        "\n",
        "### Model Evaluation and Comparison (Report Notes)\n",
        "\n",
        "The performance of the baseline machine learning model and the deep learning model was evaluated using Root Mean Squared Error (RMSE).\n",
        "The Ridge Regression baseline provided a strong reference point with stable predictive performance.\n",
        "The Multi-Layer Perceptron (MLP) achieved comparable or improved RMSE, demonstrating its ability to capture nonlinear relationships among features.\n",
        "Although the deep learning model introduces higher computational complexity, its performance benefits justify its application for large-scale student performance prediction.\n",
        "This comparison validates the progression from traditional machine learning to deep learning techniques.\n",
        "\n"
      ],
      "metadata": {
        "id": "yuTsSwEOhWvS"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 8. Kaggle Submission\n"
      ],
      "metadata": {
        "id": "N9b2BAU-J2FK"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Kaggle Submission\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "\n",
        "> Step 8.1 — Train FINAL model on full dataset\n"
      ],
      "metadata": {
        "id": "9n6xzfrm2AiA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Transform full training data and test data\n",
        "X_full_nn = preprocessor.fit_transform(X)\n",
        "X_test_nn = preprocessor.transform(test_features)\n",
        "\n",
        "print(\"Full train shape:\", X_full_nn.shape)\n",
        "print(\"Test shape:\", X_test_nn.shape)\n"
      ],
      "metadata": {
        "id": "HeIkaHJJ2AMq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "> Step 8.2 — Retrain MLP on full data\n",
        "\n"
      ],
      "metadata": {
        "id": "XZouZ5ic2Mo7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Retrain MLP on full training data\n",
        "final_mlp = Sequential([\n",
        "    Dense(128, activation=\"relu\", input_shape=(X_full_nn.shape[1],)),\n",
        "    Dropout(0.3),\n",
        "    Dense(64, activation=\"relu\"),\n",
        "    Dropout(0.3),\n",
        "    Dense(1)\n",
        "])\n",
        "\n",
        "final_mlp.compile(\n",
        "    optimizer=\"adam\",\n",
        "    loss=\"mse\"\n",
        ")\n",
        "\n",
        "final_mlp.fit(\n",
        "    X_full_nn, y,\n",
        "    epochs=15,\n",
        "    batch_size=256,\n",
        "    verbose=1\n",
        ")\n"
      ],
      "metadata": {
        "id": "uW1D_q9j2P3Z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        ">Step 8.3 — Generate predictions for Kaggle test set\n",
        "\n"
      ],
      "metadata": {
        "id": "-rJO6TGh3pv6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Predict on test data\n",
        "test_preds = final_mlp.predict(X_test_nn).ravel()\n",
        "\n",
        "test_preds[:10]\n"
      ],
      "metadata": {
        "id": "w1D7HxIe3sJT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "> Step 8.4 — Create submission file\n",
        "\n"
      ],
      "metadata": {
        "id": "lhYG9fUe3ubE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Create submission DataFrame\n",
        "submission = pd.DataFrame({\n",
        "    \"id\": test_df[\"id\"],\n",
        "    \"exam_score\": test_preds\n",
        "})\n",
        "\n",
        "# Save to CSV\n",
        "submission.to_csv(\"submission.csv\", index=False)\n",
        "\n",
        "submission.head()\n"
      ],
      "metadata": {
        "id": "aScGoyT-3wiJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "> Step 8.5 — Download and upload to Kaggle\n",
        "\n"
      ],
      "metadata": {
        "id": "2SzTass93znO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "> STEP 8 — Ready-made report text\n",
        "\n",
        "\n",
        "### Kaggle Submission (Report Notes)\n",
        "\n",
        "The final deep learning model was retrained using the complete training dataset to maximize learning capacity.\n",
        "Predictions were generated for the unseen test dataset and formatted according to the competition submission requirements.\n",
        "The resulting submission file was successfully uploaded to Kaggle, completing the end-to-end machine learning workflow from data exploration to deployment-ready prediction.\n"
      ],
      "metadata": {
        "id": "D0DZH73-3398"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 9. Save Model for Deployment\n"
      ],
      "metadata": {
        "id": "Jh2DEMXzJ3TF"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "> STEP 9 — Final Conclusion & Reflection\n",
        "\n",
        "---\n",
        "\n",
        "### Conclusion\n",
        "\n",
        "This study successfully developed and evaluated a machine learning and deep learning framework to predict student exam performance using demographic, behavioral, and educational attributes.\n",
        "Exploratory Data Analysis (EDA) revealed meaningful relationships between study habits, attendance, sleep patterns, and academic outcomes, confirming the relevance of the selected features.\n",
        "\n",
        "A structured preprocessing pipeline was implemented to handle numerical scaling and categorical encoding while preventing data leakage.\n",
        "A Ridge Regression model was first employed as a baseline to establish a reliable performance benchmark.\n",
        "Subsequently, a Multi-Layer Perceptron (MLP) deep learning model was developed to capture nonlinear relationships within the data.\n",
        "\n",
        "Evaluation using Root Mean Squared Error (RMSE) demonstrated that the deep learning model achieved comparable or improved performance relative to the baseline model.\n",
        "The results indicate that deep learning techniques can effectively model complex educational data, particularly when large datasets are available.\n",
        "Overall, the project demonstrates a complete, professional machine learning workflow from data exploration to model deployment via Kaggle submission.\n",
        "\n",
        "\n",
        "---\n",
        "### Reflection and Future Work\n",
        "\n",
        "This project provided valuable practical experience in applying both traditional machine learning and deep learning techniques to a real-world regression problem.\n",
        "One of the key learning outcomes was understanding the importance of Exploratory Data Analysis in guiding preprocessing and model selection decisions.\n",
        "The comparison between a baseline regression model and a neural network highlighted the trade-offs between model complexity, interpretability, and performance.\n",
        "\n",
        "While the deep learning model achieved strong predictive accuracy, future improvements could include hyperparameter optimization, experimentation with alternative architectures, or the use of ensemble learning methods.\n",
        "Additionally, incorporating feature engineering techniques or temporal academic data could further enhance prediction accuracy.\n",
        "\n",
        "From an ethical perspective, it is important to ensure that predictive models in educational contexts are used responsibly, avoiding bias and supporting students rather than penalizing them.\n",
        "Overall, this project strengthened both technical and analytical skills while reinforcing best practices in machine learning system development.\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "yoydi_C23_8p"
      }
    }
  ]
}